{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning MNIST and CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljjDKFGGvpzJ"
      },
      "source": [
        "#import module\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets,transforms\n",
        "import os\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6L3Dtx4Hl-T"
      },
      "source": [
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC-elaloI4Hi"
      },
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlvEDarOI69Q"
      },
      "source": [
        "#check the device is cpu or gpu\n",
        "device = get_device()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVrykbpPazke"
      },
      "source": [
        "#define defalut parameters and you can also change them as you wish\n",
        "epoch_num = 5\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "param_grid = {\n",
        "    \n",
        "    'learning_rate': [0.001,0.01],\n",
        "    'batch_size': [256],\n",
        "}\n",
        "paras = list(itertools.product(param_grid['learning_rate'], param_grid['batch_size']))\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmsHBZnjYGbg"
      },
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEItqitGv6cP"
      },
      "source": [
        "#run this cell to load MNIST Dataset \n",
        "def load_MNIST_dataset():\n",
        "    train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                                  train=True,\n",
        "                                  transform = transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1037,), (0.3081,))\n",
        "                  ]), download = True)\n",
        "    test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                                  train=False,\n",
        "                                  transform = transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1037,), (0.3081,))\n",
        "                  ]))\n",
        "    return train_dataset,test_dataset"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtQz22MxylhO"
      },
      "source": [
        "#run this cell to load cifar10 Dataset\n",
        "def load_cifar10_dataset():\n",
        "    train_dataset = datasets.CIFAR10(root='./cifar10_data/',\n",
        "                                  train=True,\n",
        "                                  transform = transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1037,), (0.3081,)),\n",
        "                      transforms.Resize(224)\n",
        "                  ]), download = True)\n",
        "    test_dataset = datasets.CIFAR10(root='./cifar10_data/',\n",
        "                                  train=False,\n",
        "                                  transform = transforms.Compose([\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.1037,), (0.3081,)),\n",
        "                      transforms.Resize(224)\n",
        "                  ]))\n",
        "    return train_dataset,test_dataset\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUa4EIUUfTCx"
      },
      "source": [
        "# Data Loader (Input Pipeline)\n",
        "def train_loader(train_dataset,batch_size):\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size= batch_size ,\n",
        "                                           shuffle=True)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def test_loader(test_dataset):\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                           batch_size= 100,\n",
        "                                           shuffle=True)\n",
        "    return test_loader\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9muElADnLyXe"
      },
      "source": [
        "#visualize the dataset\n",
        "def visualize_dataset(photo_nums,train_dataset):\n",
        "    fig = plt.figure()\n",
        "    for i in range(photo_nums):\n",
        "      plt.subplot(photo_nums,1,photo_nums+1)\n",
        "      plt.tight_layout()\n",
        "      image = (train_dataset.data[i])\n",
        "      plt.imshow(image, interpolation='none')\n",
        "      \n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "    #plt.title(\"Original Image\")\n",
        "    #plt.rcParams change the pixels of the image generated\n",
        "    plt.rcParams['savefig.dpi'] = 300 \n",
        "    plt.rcParams['figure.dpi'] = 300\n",
        "    plt.show()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Pb08222lSk"
      },
      "source": [
        "# define Networks, there are three networks defined in this code and you can define your own network if needed\n",
        "class Net(nn.Module):\n",
        "  \n",
        "     def __init__(self):\n",
        "        super().__init__()\n",
        "        #1*1*28*28\n",
        "        self.conv1 = nn.Conv2d(1, 10, 5) \n",
        "        self.conv2 = nn.Conv2d(10, 20, 3) \n",
        "        self.fc1 = nn.Linear(20 * 10 * 10, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        \n",
        "     def forward(self, x): \n",
        "        in_size = x.size(0)\n",
        "        out= self.conv1(x) # 1* 10 * 24 *24\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, 2, 2) # 1* 10 * 12 * 12\n",
        "        out = self.conv2(out) # 1* 20 * 10 * 10\n",
        "        out = F.relu(out)\n",
        "        out = out.view(in_size, -1) # 1 * 2000\n",
        "        out = self.fc1(out) # 1 * 500\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out) # 1 * 10\n",
        "        out = F.log_softmax(out, dim = 1)\n",
        "        return out\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHLXDbeNNBOd"
      },
      "source": [
        "# define Networks\n",
        "class Net_cifar(nn.Module):\n",
        "   def __init__(self):\n",
        "       super().__init__()    \n",
        "       self.conv1 = nn.Conv2d(3, 6, 3)  \n",
        "       self.pool = nn.MaxPool2d(2, 2)  #6*15*15\n",
        "       self.conv2 = nn.Conv2d(6, 16, 3) #16*13*13\n",
        "       self.fc1 = nn.Linear(16* 6 * 6, 500)  \n",
        "       self.fc2 = nn.Linear(500, 200)      \n",
        "       self.fc3 = nn.Linear(200, 10)     \n",
        "\n",
        "   def forward(self, x):       \n",
        "       x = self.pool(F.relu(self.conv1(x)))  \n",
        "       x = self.pool(F.relu(self.conv2(x)))   \n",
        "       x = x.view(-1, 16* 6* 6)     \n",
        "       x = F.relu(self.fc1(x))       \n",
        "       x = F.relu(self.fc2(x))       \n",
        "       x = self.fc3(x)                   \n",
        "       return x                   \n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HB1AedUiH5_"
      },
      "source": [
        "# define Networks\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        #self.conv = nn.Conv2d(1, 3, kernel_size=1)\n",
        "        self.resnet = torchvision.models.resnet18(pretrained=False)\n",
        " \n",
        "    def forward(self, x):\n",
        "        #x= self.conv(x)\n",
        "        x= self.resnet(x)\n",
        "        return x\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B68WTADtknC"
      },
      "source": [
        "def train(train_set, model, batch_size, learning_rate,loss_func):\n",
        "\n",
        "    model.train()\n",
        "    all_loss = 0\n",
        "    i = 0\n",
        "  \n",
        "    for data,target in train_set:\n",
        "        \n",
        "        data,target = Variable(data).to(device), Variable(target).to(device)\n",
        "                \n",
        "        opt.zero_grad()\n",
        "        output = model(data)\n",
        "        \n",
        "        loss = loss_func(output, target)\n",
        "        all_loss += loss\n",
        "        #backpropagation\n",
        "        loss.backward()\n",
        "        #update parmeters\n",
        "        opt.step()\n",
        "        i = i+len(data)\n",
        "\n",
        "    print(i)\n",
        "    ave_loss = all_loss/i\n",
        "    print('Epoch {}, training loss is {}'.format(epoch,ave_loss))\n",
        "    return ave_loss\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hDzm1Gw36Us"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDZuVDemfNH"
      },
      "source": [
        "\n",
        "def test(test_set,model,loss_func):\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        i=0\n",
        "        for data,target in test_set:\n",
        "\n",
        "            data, target = Variable(data,volatile=True).to(device) ,Variable(target).to(device)\n",
        "            output = model(data)\n",
        "            loss = loss_func(output, target)\n",
        "            test_loss += loss.item() \n",
        "            pred = torch.max(output.data,1)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            i = i+len(data)\n",
        "                    \n",
        "        acc = correct/i\n",
        "        print(i,correct)\n",
        "\n",
        "        \n",
        "        ave_test_loss = test_loss/i\n",
        "        print('test loss is {}, accuracy is {}'.format(ave_test_loss,acc))\n",
        "\n",
        "        return acc,ave_test_loss\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf7ik_QcPiSb"
      },
      "source": [
        "# choose dataset and model\n",
        "train_dataset, test_dataset = load_MNIST_dataset()\n",
        "model = Net()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqohYfA8boto",
        "outputId": "c7ee2f9c-5432-4860-bd0b-d5340e31bf8b"
      },
      "source": [
        "import time\n",
        "#set_seed(24)\n",
        "\n",
        "all_time = []\n",
        "hyper_train_loss = []\n",
        "hyper_test_loss = []\n",
        "hyper_test_score = []\n",
        "all_test_score = 0\n",
        "for learning_rate in param_grid['learning_rate']:\n",
        "    for batch_size in param_grid['batch_size']:\n",
        "\n",
        "        model = model.to(get_device())\n",
        "        opt = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "        train_set = train_loader(train_dataset,batch_size)\n",
        "        test_set = test_loader(test_dataset)\n",
        "\n",
        "        print('lr is {}, batch size is {}'.format(learning_rate,batch_size))\n",
        "        start = time.time()\n",
        "        i = 0\n",
        "        \n",
        "        for epoch in range(1, epoch_num):\n",
        "            train_loss = train(train_set,model,batch_size, learning_rate,loss_func)\n",
        "            test_score, test_loss  = test(test_set,model,loss_func)\n",
        "\n",
        "            hyper_train_loss.append(train_loss)\n",
        "            hyper_test_loss.append(test_loss)\n",
        "            hyper_test_score.append(test_score)\n",
        "            \n",
        "            all_test_score += test_score\n",
        "            i += 1\n",
        "            \n",
        "        ave_all_test_score = all_test_score/i\n",
        "            \n",
        "        end = time.time()\n",
        "        t_each = end-start\n",
        "        print('run time for lr {},batch size {} is {}'.format(learning_rate,batch_size,t_each))\n",
        "        all_time.append(t_each)\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr is 0.001, batch size is 256\n",
            "60000\n",
            "Epoch 1, training loss is 0.0009132426930591464\n",
            "10000 tensor(9766, device='cuda:0')\n",
            "test loss is 0.0007698341075330973, accuracy is 0.9765999913215637\n",
            "60000\n",
            "Epoch 2, training loss is 0.0002644273918122053\n",
            "10000 tensor(9863, device='cuda:0')\n",
            "test loss is 0.00041218348164111376, accuracy is 0.986299991607666\n",
            "60000\n",
            "Epoch 3, training loss is 0.00017757841851562262\n",
            "10000 tensor(9888, device='cuda:0')\n",
            "test loss is 0.00033281968769151716, accuracy is 0.9887999892234802\n",
            "60000\n",
            "Epoch 4, training loss is 0.00012990704271942377\n",
            "10000 tensor(9871, device='cuda:0')\n",
            "test loss is 0.0004205395473050885, accuracy is 0.9870999455451965\n",
            "run time for lr 0.001,batch size 256 is 62.66845941543579\n",
            "lr is 0.01, batch size is 256\n",
            "60000\n",
            "Epoch 1, training loss is 0.001369639066979289\n",
            "10000 tensor(9792, device='cuda:0')\n",
            "test loss is 0.0006198435838334263, accuracy is 0.9791999459266663\n",
            "60000\n",
            "Epoch 2, training loss is 0.0002086363674607128\n",
            "10000 tensor(9838, device='cuda:0')\n",
            "test loss is 0.0004896130056818947, accuracy is 0.9837999939918518\n",
            "60000\n",
            "Epoch 3, training loss is 0.00016575842164456844\n",
            "10000 tensor(9840, device='cuda:0')\n",
            "test loss is 0.0004783737638732418, accuracy is 0.9839999675750732\n",
            "60000\n",
            "Epoch 4, training loss is 0.00012272628373466432\n",
            "10000 tensor(9831, device='cuda:0')\n",
            "test loss is 0.0005832289899874013, accuracy is 0.9830999970436096\n",
            "run time for lr 0.01,batch size 256 is 62.737207889556885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEBZPCRResjk"
      },
      "source": [
        "plot_train_loss = []\n",
        "plot_test_loss = hyper_test_loss\n",
        "plot_test_score = []\n",
        "for num in hyper_train_loss:\n",
        "    plot_train_loss.append(num.item())\n",
        "for num in hyper_test_score:\n",
        "    plot_test_score.append(num.item())\n",
        "all_color = ['darkblue','blue','lightblue','darkred','red','tomato','darkgreen','limegreen','lightgreen']"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chej-OcKAprI"
      },
      "source": [
        "#plot training loss\n",
        "fig = plt.figure()\n",
        "for i in range(len(paras)):\n",
        "    plt.plot(range(1, epoch_num), plot_train_loss[i*(epoch_num-1):((i*epoch_num)+epoch_num-1-i)], color=all_color[i],alpha=0.8,label=paras[i])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks(range(1,epoch_num))\n",
        "plt.legend(loc=1,fontsize=6)\n",
        "plt.title('Training loss')\n",
        "plt.rcParams['savefig.dpi'] = 300 \n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_eRI-VBDqP5"
      },
      "source": [
        "#plot testing loss\n",
        "fig = plt.figure()\n",
        "for i in range(len(paras)):\n",
        "    plt.plot(range(1, epoch_num), plot_test_loss[i*(epoch_num-1):((i*epoch_num)+epoch_num-1-i)], color=all_color[i],alpha=0.8,label=paras[i])\n",
        "plt.xlabel('epoch')  \n",
        "plt.ylabel('loss')\n",
        "plt.xticks(range(1,epoch_num))\n",
        "plt.legend(loc=1,fontsize=6)\n",
        "plt.title('Testing loss')\n",
        "plt.rcParams['savefig.dpi'] = 300 \n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-cE78OFQ9I"
      },
      "source": [
        "#plot test score\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(len(paras)):\n",
        "    plt.plot(range(1, epoch_num), plot_test_score[i*(epoch_num-1):((i*epoch_num)+epoch_num-1-i)], color=all_color[i],alpha=0.8,label=paras[i])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xticks(range(1,epoch_num))\n",
        "plt.legend(loc=4,fontsize=6)\n",
        "plt.title('Testing score')\n",
        "#plt.rcParams['savefig.dpi'] = 300 \n",
        "#plt.rcParams['figure.dpi'] = 300\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xxm174pfpls"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    }
  ]
}